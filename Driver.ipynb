{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGInitializer Driver\n",
    "from RAGInitializer import LLMType, EmbeddingType, RAGConfig, initialize_rag_components\n",
    "\n",
    "'''\n",
    "Available choices for language models (LLMs)\n",
    "GPT: \n",
    "    0 - gpt-4o\n",
    "Ollama:\n",
    "    0 - llama3.1:8b-instruct-q5_K_M\n",
    "    1 - llama3.2:latest\n",
    "    2 - mistral-nemo:12b-instruct-2407-q5_K_M\n",
    "\n",
    "Available choices for embedding models\n",
    "GPT:\n",
    "    0 - text-embedding-3-small\n",
    "    1 - text-embedding-3-large\n",
    "Ollama:\n",
    "    0 - nomic-embed-text\n",
    "    1 - mxbai-embed-large\n",
    "    2 - all-minilm\n",
    "    3 - snowflake-arctic-embed\n",
    "Sentence Transformer:\n",
    "    0 - all-MiniLM-L6-v2\n",
    "    1 - all-MiniLM-L12-v2\n",
    "    2 - all-mpnet-base-v2\n",
    "    3 - all-distilbert-base-v2\n",
    "    4 - multi-qa-mpnet-base-dot-v1\n",
    "'''\n",
    "\n",
    "# Initialize with configuration\n",
    "config = RAGConfig(\n",
    "    env_path = r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\EDQP_RAG_Model\\env_variables.env\",\n",
    "    llm_type=LLMType.OLLAMA, # GPT or OLLAMA\n",
    "    embedding_type=EmbeddingType.SENTENCE_TRANSFORMER, # OLLAMA, GPT, or SENTENCE_TRANSFORMER\n",
    "    llm_index=1,\n",
    "    embedding_index=2\n",
    ")\n",
    "\n",
    "model, embeddings, dimensions, selected_llm, selected_embedding_model, model_manager = initialize_rag_components(config)\n",
    "if model and embeddings and dimensions:\n",
    "    print(\"\\nRAG components successfully initialized.\")\n",
    "    print(f\"Model: {selected_llm}\")\n",
    "    print(f\"Embeddings: {selected_embedding_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChunkingInitializer Driver\n",
    "\n",
    "from ChunkingInitializer import ChunkingInitializer\n",
    "from ChunkingMethod import ChunkingMethod\n",
    "import sys\n",
    "\n",
    "processor = ChunkingInitializer(\n",
    "    source_path=r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\Source_Documents\\MCDP_1_Warfighting.pdf\",\n",
    "    chunking_method=ChunkingMethod.PAGE,\n",
    "    enable_preprocessing=False,\n",
    "    model_name=selected_llm,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "try:\n",
    "    documents = processor.process()\n",
    "    \n",
    "    # Output results\n",
    "    for doc in documents:\n",
    "        print(f\"Chunk metadata: {doc.metadata}\")\n",
    "        print(f\"Chunk content: {doc.page_content[:200]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Processing failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if documents:\n",
    "    print(\"\\nDocument processing completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatastoreInitializer Driver\n",
    "\n",
    "import sys\n",
    "from DatastoreInitializer import DatastoreInitializer, StorageType\n",
    "\n",
    "datastore_manager = DatastoreInitializer(\n",
    "    doc_name='test-datastore',\n",
    "    pinecone_api_key=model_manager.get_pinecone_api_key(),\n",
    "    dimensions=dimensions,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Set up datastore\n",
    "    datastore = datastore_manager.setup_datastore(\n",
    "        storage_type=StorageType.PINECONE_EXISTING,\n",
    "        documents=documents,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionInitializer Driver\n",
    "\n",
    "from QuestionInitializer import QuestionInitializer\n",
    "\n",
    "# QuestionInitializer Driver\n",
    "processor = QuestionInitializer(\n",
    "    datastore=datastore,\n",
    "    model=model,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "# Define questions\n",
    "template_name = \"detailed\"  # default, short, or detailed\n",
    "questions = [\"According to the text, what is the definition of war?\",\n",
    "             \"What does the text say about uncertainty in warfare?\",\n",
    "             \"How does the text define the spectrum of conflict? What are the various factors involved?\",\n",
    "             \"What are centers of gravity and critical vulnerabilities in warfare?\"\n",
    "            ]\n",
    "\n",
    "try:\n",
    "    # Process questions and get execution time\n",
    "    processing_time = processor.process_questions(\n",
    "        questions=questions,\n",
    "        use_ground_truth=False,\n",
    "        template_name=template_name  # Added template parameter\n",
    "    )\n",
    "    print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
