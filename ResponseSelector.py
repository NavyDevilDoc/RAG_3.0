from ScoringMetric import ScoringMetric
from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer, AutoTokenizer, AutoModel
import torch
import pandas as pd
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer, util
from typing import List, Tuple
from functools import lru_cache

class ResponseSelector:
    REASONING_MARKERS = ['because', 'therefore', 'however']
    RELEVANCE_WEIGHT = 0.4
    CONFIDENCE_WEIGHT = 0.3
    REASONING_WEIGHT = 0.2
    COHERENCE_WEIGHT = 0.1

    @lru_cache(maxsize=1000)
    def _cached_embedding(self, text: str) -> np.ndarray:
        """Cache embeddings for frequently seen text."""
        return self.embedding_model.encode(text)

    def __init__(self, model_name: str = "all-mpnet-base-v2", 
                 top_results: int = 15, 
                 use_reranking: bool = True, 
                 save_outputs: bool = False, 
                 output_file_path: str = "re-ranking_test_outputs.txt"):
        """Initialize response selector with embedding model and top_k parameter."""
        try:
            self.embedding_model = SentenceTransformer(model_name)
        except Exception as e:
            raise ValueError(f"Error loading embedding model '{model_name}': {e}")
        if top_results <= 0:
            raise ValueError("top_k must be a positive integer.")
        self.top_results = top_results
        self.use_reranking = use_reranking
        self.save_outputs = save_outputs
        self.output_file_path = output_file_path
        self.scoring_metric = ScoringMetric(model_name, 'sentence_transformer')
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')


    def _get_relevance_score(self, question: str, response: str) -> float:
        """Calculate semantic similarity score between question and response."""
        if not question or not response:
            return 0.0
        try:
            q_embedding = self._cached_embedding(question)
            r_embedding = self._cached_embedding(response)
            return float(util.pytorch_cos_sim(q_embedding, r_embedding))
        except Exception as e:
            print(f"Error calculating relevance score: {e}")
            return 0.0

    
    # Rank responses generated by the language model by combining relevance and confidence scores
    def rank_responses(self, question: str, responses: List[str]) -> List[Tuple[str, float]]:
        """Rank responses by combining relevance and confidence scores."""
        if not responses:
            raise ValueError("The 'responses' list cannot be empty.")
        scored_responses = []
        for response in responses:
            try:
                relevance = self._get_relevance_score(question, response)
                confidence = self.scoring_metric.calculate_confidence(response, question)
                reasoning = sum(1 for marker in self.REASONING_MARKERS if marker in response.lower()) / len(self.REASONING_MARKERS)
                coherence = self._check_response_coherence(response)
                
                final_score = (
                    self.RELEVANCE_WEIGHT * relevance +
                    self.CONFIDENCE_WEIGHT * confidence +
                    self.REASONING_WEIGHT * reasoning +
                    self.COHERENCE_WEIGHT * coherence
                )
                scored_responses.append((response, final_score))
            except Exception as e:
                print(f"Error ranking response: {e}")
        # Sort and return only the top n responses
        try:
            return sorted(scored_responses, key=lambda x: x[1], reverse=True)[:self.top_results]
        except Exception as e:
            print(f"Error sorting responses: {e}")
            return []

    '''
    def rank_responses(self, question: str, responses: List[str]) -> List[Tuple[str, float]]:
        scored_responses = []
        for response in responses:
            try:
                relevance = self._get_relevance_score(question, response)
                confidence = self.scoring_metric.calculate_confidence(response, question)
                reasoning = sum(1 for marker in self.REASONING_MARKERS if marker in response.lower()) / len(self.REASONING_MARKERS)
                coherence = self._check_response_coherence(response)
                
                final_score = (
                    self.RELEVANCE_WEIGHT * relevance +
                    self.CONFIDENCE_WEIGHT * confidence +
                    self.REASONING_WEIGHT * reasoning +
                    self.COHERENCE_WEIGHT * coherence
                )
                scored_responses.append((response, final_score))
            except Exception as e:
                print(f"Error ranking response: {e}")
        
        return sorted(scored_responses, key=lambda x: x[1], reverse=True)[:self.top_results]
    '''


    def _check_response_coherence(self, response: str) -> float:
        """Check response coherence using sentence embeddings"""
        sentences = response.split('.')
        if len(sentences) < 2:
            return 1.0
        
        embeddings = [self._cached_embedding(sent) for sent in sentences if sent.strip()]
        similarities = []
        
        for i in range(len(embeddings)-1):
            sim = util.pytorch_cos_sim(embeddings[i], embeddings[i+1])
            similarities.append(float(sim))
        
        return sum(similarities) / len(similarities)


    def _split_into_chunks(self, text: str, max_length: int) -> List[str]:
        """Split text into chunks of max_length tokens using RecursiveCharacterTextSplitter."""
        splitter = RecursiveCharacterTextSplitter(chunk_size=max_length, chunk_overlap=0)
        chunks = splitter.split_text(text)
        return chunks


    def _attention_weighted_mean(self, chunk_embeddings: torch.Tensor, query_embedding: torch.Tensor) -> torch.Tensor:
        """Calculate attention-weighted mean of chunk embeddings based on query relevance."""
        # Calculate attention scores using scaled dot product attention
        scaling_factor = torch.sqrt(torch.tensor(chunk_embeddings.size(-1), dtype=torch.float32))
        attention_scores = torch.matmul(chunk_embeddings, query_embedding.T) / scaling_factor
        
        # Apply softmax to get attention weights
        attention_weights = torch.nn.functional.softmax(attention_scores, dim=0)
        
        # Calculate weighted mean
        weighted_embeddings = chunk_embeddings * attention_weights
        return torch.sum(weighted_embeddings, dim=0)


    def rerank_documents(self, query: str, documents: List[str]) -> List[Tuple[str, float]]:
        """Re-rank documents using attention-weighted BERT embeddings."""
        BATCH_SIZE = 8
        query_embedding = self._encode_text(query)
        
        ranked_docs = []
        for i in range(0, len(documents), BATCH_SIZE):
            batch_docs = documents[i:i + BATCH_SIZE]
            batch_embeddings = []
            
            for doc in batch_docs:
                # Split document into chunks
                chunks = self._split_into_chunks(doc, max_length=512)
                if not chunks:
                    continue
                    
                # Get embeddings for all chunks in the document
                chunk_embeddings = torch.stack([self._encode_text(chunk) for chunk in chunks])
                
                # Calculate attention-weighted document embedding
                doc_embedding = self._attention_weighted_mean(chunk_embeddings, query_embedding)
                batch_embeddings.append((doc, doc_embedding))
            
            # Calculate similarities for the batch
            for doc, doc_embedding in batch_embeddings:
                similarity = self._cosine_similarity(query_embedding, doc_embedding)
                ranked_docs.append((doc, similarity))
        
        # Sort by similarity scores
        return sorted(ranked_docs, key=lambda x: x[1], reverse=True)




    def _encode_text(self, text: str) -> torch.Tensor:
        """Encode text using BERT."""
        inputs = self.tokenizer(text, return_tensors='pt')
        outputs = self.bert_model(**inputs)
        return outputs.last_hidden_state.mean(dim=1)


    def _cosine_similarity(self, tensor1: torch.Tensor, tensor2: torch.Tensor) -> float:
        """Compute cosine similarity between two tensors."""
        return torch.nn.functional.cosine_similarity(tensor1, tensor2).item()


    def save_outputs_to_file(self, original_outputs: List[str], reranked_outputs: List[str]):
        """Save original and re-ranked outputs to a text file."""
        with open(self.output_file_path, "w") as file:
            file.write("Original Outputs:\n")
            for output in original_outputs:
                file.write(output + "\n")
            file.write("\nRe-ranked Outputs:\n")
            for output in reranked_outputs:
                file.write(output + "\n")


    def select_best_response(self, ranked_responses: List[Tuple[str, float]]) -> str:
        """Select single best response from pre-ranked candidates."""
        try:
            return ranked_responses[0][0] if ranked_responses else "No suitable response found."
        except Exception as e:
            print(f"Error selecting best response: {e}")
            return "No suitable response found."
        

    def save_results_to_dataframe(self, original_results, re_ranked_results, responses):
        """Save original, re-ranked results, and responses to a DataFrame and save to a CSV file."""
        original_results_len = len(original_results)
        re_ranked_results_len = len(re_ranked_results)
        responses_len = len(responses)

        max_len = max(original_results_len, re_ranked_results_len, responses_len)

        # Extend lists to the maximum length by filling with None
        original_results.extend([("", None)] * (max_len - original_results_len))
        re_ranked_results.extend([("", None)] * (max_len - re_ranked_results_len))
        responses.extend([("", None)] * (max_len - responses_len))

        data = {
            "Original Result": [result[0] for result in original_results],
            "Original Score": [result[1] for result in original_results],
            "Re-ranked Result": [result[0] for result in re_ranked_results],
            "Re-ranked Score": [result[1] for result in re_ranked_results],
            "Response": [response[0] for response in responses],
            "Response Score": [response[1] for response in responses]
        }

        df = pd.DataFrame(data)
        return df