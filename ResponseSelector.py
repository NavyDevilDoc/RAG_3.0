from ScoringMetric import ScoringMetric
from transformers import BertTokenizer, BertModel, RobertaModel, RobertaTokenizer, AutoTokenizer, AutoModel
import torch
import pandas as pd
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer, util
from typing import List, Tuple
from functools import lru_cache

class ResponseSelector:
    REASONING_MARKERS = ['because', 'therefore', 'however']
    RELEVANCE_WEIGHT = 0.7
    CONFIDENCE_WEIGHT = 0.3

    @lru_cache(maxsize=1000)
    def _cached_embedding(self, text: str) -> np.ndarray:
        """Cache embeddings for frequently seen text."""
        return self.embedding_model.encode(text)

    def __init__(self, model_name: str = "all-mpnet-base-v2", 
                 top_results: int = 15, 
                 use_reranking: bool = True, 
                 save_outputs: bool = False, 
                 output_file_path: str = "re-ranking_test_outputs.txt"):
        """Initialize response selector with embedding model and top_k parameter."""
        try:
            self.embedding_model = SentenceTransformer(model_name)
        except Exception as e:
            raise ValueError(f"Error loading embedding model '{model_name}': {e}")
        if top_results <= 0:
            raise ValueError("top_k must be a positive integer.")
        self.top_results = top_results
        self.use_reranking = use_reranking
        self.save_outputs = save_outputs
        self.output_file_path = output_file_path
        self.scoring_metric = ScoringMetric(model_name, 'sentence_transformer')
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')


    def _get_relevance_score(self, question: str, response: str) -> float:
        """Calculate semantic similarity score between question and response."""
        if not question or not response:
            return 0.0
        try:
            q_embedding = self._cached_embedding(question)
            r_embedding = self._cached_embedding(response)
            return float(util.pytorch_cos_sim(q_embedding, r_embedding))
        except Exception as e:
            print(f"Error calculating relevance score: {e}")
            return 0.0


    # Rank responses generated by the language model by combining relevance and confidence scores
    def rank_responses(self, question: str, responses: List[str]) -> List[Tuple[str, float]]:
        """Rank responses by combining relevance and confidence scores."""
        if not responses:
            raise ValueError("The 'responses' list cannot be empty.")
        scored_responses = []
        for response in responses:
            try:
                relevance = self._get_relevance_score(question, response)
                confidence = self.scoring_metric.calculate_confidence(response, question)
                final_score = self.RELEVANCE_WEIGHT * relevance + self.CONFIDENCE_WEIGHT * confidence
                scored_responses.append((response, final_score))
            except Exception as e:
                print(f"Error ranking response: {e}")
        scores = [score for _, score in scored_responses]
        scored_responses = [(response, score) for (response, _), score in zip(scored_responses, scores)]
        # Sort and return only the top n responses
        try:
            return sorted(scored_responses, key=lambda x: x[1], reverse=True)[:self.top_results]
        except Exception as e:
            print(f"Error sorting responses: {e}")
            return []


    def _split_into_chunks(self, text: str, max_length: int) -> List[str]:
        """Split text into chunks of max_length tokens using RecursiveCharacterTextSplitter."""
        splitter = RecursiveCharacterTextSplitter(chunk_size=max_length, chunk_overlap=0)
        chunks = splitter.split_text(text)
        return chunks


    # Use a BERT-based model to re-rank the top documents retrieved by Pinecone
    def rerank_documents(self, query: str, documents: List[str]) -> List[Tuple[str, float]]:
        """Re-rank documents using BERT-based similarity."""
        query_embedding = self._encode_text(query)
        doc_embeddings = []
        for doc in documents:
            chunks = self._split_into_chunks(doc, max_length=512)
            chunk_embeddings = [self._encode_text(chunk) for chunk in chunks]
            doc_embedding = torch.mean(torch.stack(chunk_embeddings), dim=0)
            doc_embeddings.append(doc_embedding)
        similarities = [self._cosine_similarity(query_embedding, doc_embedding) for doc_embedding in doc_embeddings]
        ranked_docs_with_scores = sorted(zip(similarities, documents), key=lambda x: x[0], reverse=True)
        ranked_docs = [(doc, score) for score, doc in ranked_docs_with_scores]
        return ranked_docs


    def _encode_text(self, text: str) -> torch.Tensor:
        """Encode text using BERT."""
        inputs = self.tokenizer(text, return_tensors='pt')
        outputs = self.bert_model(**inputs)
        return outputs.last_hidden_state.mean(dim=1)


    def _cosine_similarity(self, tensor1: torch.Tensor, tensor2: torch.Tensor) -> float:
        """Compute cosine similarity between two tensors."""
        return torch.nn.functional.cosine_similarity(tensor1, tensor2).item()


    def save_outputs_to_file(self, original_outputs: List[str], reranked_outputs: List[str]):
        """Save original and re-ranked outputs to a text file."""
        with open(self.output_file_path, "w") as file:
            file.write("Original Outputs:\n")
            for output in original_outputs:
                file.write(output + "\n")
            file.write("\nRe-ranked Outputs:\n")
            for output in reranked_outputs:
                file.write(output + "\n")


    def select_best_response(self, ranked_responses: List[Tuple[str, float]]) -> str:
        """Select single best response from pre-ranked candidates."""
        try:
            return ranked_responses[0][0] if ranked_responses else "No suitable response found."
        except Exception as e:
            print(f"Error selecting best response: {e}")
            return "No suitable response found."
        

    def save_results_to_dataframe(self, original_results, re_ranked_results, responses):
        """Save original, re-ranked results, and responses to a DataFrame and save to a CSV file."""
        original_results_len = len(original_results)
        re_ranked_results_len = len(re_ranked_results)
        responses_len = len(responses)

        max_len = max(original_results_len, re_ranked_results_len, responses_len)

        # Extend lists to the maximum length by filling with None
        original_results.extend([("", None)] * (max_len - original_results_len))
        re_ranked_results.extend([("", None)] * (max_len - re_ranked_results_len))
        responses.extend([("", None)] * (max_len - responses_len))

        data = {
            "Original Result": [result[0] for result in original_results],
            "Original Score": [result[1] for result in original_results],
            "Re-ranked Result": [result[0] for result in re_ranked_results],
            "Re-ranked Score": [result[1] for result in re_ranked_results],
            "Response": [response[0] for response in responses],
            "Response Score": [response[1] for response in responses]
        }

        df = pd.DataFrame(data)
        return df