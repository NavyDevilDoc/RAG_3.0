{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "navsea-org-best-text-embedding-3-large:\n",
    "\n",
    "PPBE\n",
    "NAVSEA ORGANIZATION\n",
    "JCIDS\n",
    "FLEET MAINTENANCE POLICY\n",
    "SYSTEM ENGINEERING OVERVIEW\n",
    "EVM\n",
    "ACQUISITION MILESTONES AND PHASES\n",
    "WARFARE CENTERS\n",
    "COST ESTIMATION\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Stable backup Python files '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGInitializer.py (Test)\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Any\n",
    "from ModelManager import ModelManager\n",
    "from ComputeResourceManager import ComputeResourceManager\n",
    "\n",
    "class LLMType(Enum):\n",
    "    GPT = \"gpt\"\n",
    "    OLLAMA = \"ollama\"\n",
    "\n",
    "class EmbeddingType(Enum):\n",
    "    GPT = \"gpt\" \n",
    "    OLLAMA = \"ollama\"\n",
    "    SENTENCE_TRANSFORMER = \"sentence_transformer\"\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    env_path: str\n",
    "    llm_type: LLMType\n",
    "    embedding_type: EmbeddingType\n",
    "    llm_index: int\n",
    "    embedding_index: int\n",
    "\n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"Convert config to format expected by ModelManager\"\"\"\n",
    "        return {\n",
    "            \"selected_llm_type\": self.llm_type.value,\n",
    "            \"selected_embedding_scheme\": self.embedding_type.value\n",
    "        }\n",
    "\n",
    "def initialize_rag_components(config: RAGConfig) -> Tuple[Any, Any, int]:\n",
    "    \"\"\"Initialize RAG components using ModelManager's existing methods\"\"\"\n",
    "    try:\n",
    "        model_manager = ModelManager(config.env_path)\n",
    "        resource_manager = ComputeResourceManager().get_compute_settings()\n",
    "        \n",
    "        # Use existing validate_and_load_models method\n",
    "        model, embeddings, dimensions, selected_llm, selected_embedding_model = model_manager.validate_and_load_models(\n",
    "            config=config.to_dict(),\n",
    "            select_llm=config.llm_index,\n",
    "            select_embed=config.embedding_index,\n",
    "            resource_manager=resource_manager\n",
    "        )\n",
    "        \n",
    "        return model, embeddings, dimensions, selected_llm, selected_embedding_model, model_manager\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to initialize RAG components: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChunkingInitializer.py (Test)\n",
    "from typing import List, Optional\n",
    "import sys\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from OCREnhancedPDFLoader import OCREnhancedPDFLoader\n",
    "from TextPreprocessor import TextPreprocessor\n",
    "from ChunkingManager import ChunkingMethod, process_document\n",
    "\n",
    "class ChunkingInitializer:\n",
    "    \"\"\"Orchestrates document processing workflow including OCR, preprocessing, and chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 source_path: str,\n",
    "                 chunking_method: ChunkingMethod = ChunkingMethod.PAGE,\n",
    "                 enable_preprocessing: bool = False,\n",
    "                 chunk_size: int = 500,\n",
    "                 chunk_overlap: int = 50,\n",
    "                 similarity_threshold: float = 0.85,\n",
    "                 model_name: Optional[str] = None,\n",
    "                 embedding_model: Optional[any] = None):\n",
    "        \"\"\"\n",
    "        Initialize chunking processor with configuration parameters.\n",
    "        \"\"\"\n",
    "        self.source_path = source_path\n",
    "        self.chunking_method = chunking_method\n",
    "        self.enable_preprocessing = enable_preprocessing\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.model_name = model_name\n",
    "        self.embedding_model = embedding_model or self._setup_default_embedding()\n",
    "        \n",
    "    def _setup_default_embedding(self) -> any:\n",
    "        \"\"\"Setup default embedding model based on chunking method.\"\"\"\n",
    "        if self.chunking_method == ChunkingMethod.SEMANTIC:\n",
    "            return SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "        return None\n",
    "\n",
    "    def _load_documents(self) -> List[Document]:\n",
    "        \"\"\"Load documents with OCR enhancement.\"\"\"\n",
    "        try:\n",
    "            print(\"Loading documents with OCR enhancement...\")\n",
    "            loader = OCREnhancedPDFLoader(self.source_path)\n",
    "            documents = loader.load()\n",
    "            print(f\"Loaded {len(documents)} documents with OCR enhancement\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _preprocess_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Apply preprocessing to documents if enabled.\"\"\"\n",
    "        try:\n",
    "            if self.enable_preprocessing:\n",
    "                print(\"Preprocessing documents...\")\n",
    "                preprocessor = TextPreprocessor()\n",
    "                return [\n",
    "                    Document(\n",
    "                        page_content=preprocessor.preprocess(doc.page_content),\n",
    "                        metadata={**doc.metadata, \"preprocessing\": \"applied\"}\n",
    "                    ) for doc in documents\n",
    "                ]\n",
    "            else:\n",
    "                print(\"Skipping preprocessing...\")\n",
    "                return [\n",
    "                    Document(\n",
    "                        page_content=doc.page_content,\n",
    "                        metadata={**doc.metadata, \"preprocessing\": \"skipped\"}\n",
    "                    ) for doc in documents\n",
    "                ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in document preprocessing: {e}\")\n",
    "            raise\n",
    "\n",
    "    def process(self) -> List[Document]:\n",
    "        \"\"\"Execute the complete document processing pipeline.\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess documents\n",
    "            raw_documents = self._load_documents()\n",
    "            processed_documents = self._preprocess_documents(raw_documents)\n",
    "            \n",
    "            # Process documents using specified chunking method\n",
    "            documents = process_document(\n",
    "                source_path=self.source_path,\n",
    "                method=self.chunking_method,\n",
    "                enable_preprocessing=self.enable_preprocessing,\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "                similarity_threshold=self.similarity_threshold,\n",
    "                model_name=self.model_name,\n",
    "                embedding_model=self.embedding_model\n",
    "            )\n",
    "            \n",
    "            print(f\"Processed {len(documents)} document chunks\")\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in document processing pipeline: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatastoreInitializer.py (Test)\n",
    "from enum import Enum\n",
    "from typing import Optional, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from PineconeManager import PineconeManager\n",
    "\n",
    "class StorageType(Enum):\n",
    "    PINECONE_NEW = 0\n",
    "    PINECONE_ADD = 1\n",
    "    PINECONE_EXISTING = 2\n",
    "    LOCAL_STORAGE = 3\n",
    "\n",
    "class DatastoreInitializer:\n",
    "    \"\"\"Manages datastore setup and configuration for vector storage.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 doc_name: str,\n",
    "                 pinecone_api_key: str,\n",
    "                 dimensions: int,\n",
    "                 embedding_model: Any):\n",
    "        \"\"\"Initialize datastore manager with configuration.\"\"\"\n",
    "        self.doc_name = doc_name\n",
    "        self.pinecone_api_key = pinecone_api_key\n",
    "        self.dimensions = dimensions\n",
    "        self.embedding_model = embedding_model\n",
    "        self.pinecone_client = None\n",
    "        self.manager = None\n",
    "        \n",
    "    def initialize_pinecone(self):\n",
    "        \"\"\"Initialize Pinecone client and manager.\"\"\"\n",
    "        try:\n",
    "            self.pinecone_client = Pinecone(api_key=self.pinecone_api_key)\n",
    "            self.manager = PineconeManager(\n",
    "                self.pinecone_client,\n",
    "                self.pinecone_api_key,\n",
    "                self.dimensions,\n",
    "                self.embedding_model\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize Pinecone: {e}\")\n",
    "\n",
    "    def _get_index_name(self) -> str:\n",
    "        \"\"\"Generate unique index name.\"\"\"\n",
    "        return f\"{self.doc_name}-{self.embedding_model}\".lower()\n",
    "\n",
    "    def setup_datastore(self, \n",
    "                       storage_type: StorageType,\n",
    "                       documents: Optional[list] = None,\n",
    "                       embeddings: Optional[Any] = None) -> Any:\n",
    "        \"\"\"Set up and return configured datastore.\"\"\"\n",
    "        try:\n",
    "            # Initialize Pinecone if not already done\n",
    "            if not self.pinecone_client:\n",
    "                self.initialize_pinecone()\n",
    "                \n",
    "            index_name = self._get_index_name()\n",
    "            print(f\"Active Index: {index_name}\")\n",
    "\n",
    "            # Handle new Pinecone index creation\n",
    "            if storage_type == StorageType.PINECONE_NEW:\n",
    "                spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "                self.manager.setup_index(index_name, spec)\n",
    "\n",
    "            # Set up datastore using manager\n",
    "            datastore = self.manager.setup_datastore(\n",
    "                storage_type.value,\n",
    "                documents,\n",
    "                embeddings,\n",
    "                index_name\n",
    "            )\n",
    "            \n",
    "            return datastore\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to setup datastore: {e}\")\n",
    "        \n",
    "    def initialize(self, storage_type: StorageType) -> Any:\n",
    "        \"\"\"Initialize and configure datastore\"\"\"\n",
    "        try:\n",
    "            # Setup Pinecone client and manager\n",
    "            self.pinecone_client = Pinecone(api_key=self.pinecone_api_key)\n",
    "            self.manager = PineconeManager(\n",
    "                self.pinecone_client,\n",
    "                self.pinecone_api_key,\n",
    "                self.dimensions,\n",
    "                self.embedding_model\n",
    "            )\n",
    "\n",
    "            # Configure index\n",
    "            index_name = self._get_index_name()\n",
    "            print(f\"Active Index: {index_name}\")\n",
    "\n",
    "            # Create new index if needed\n",
    "            if storage_type == StorageType.NEW:\n",
    "                spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "                self.manager.setup_index(index_name, spec)\n",
    "\n",
    "            # Initialize documents for existing index\n",
    "            documents = None if storage_type == StorageType.EXISTING else []\n",
    "\n",
    "            # Setup and return datastore\n",
    "            return self.manager.setup_datastore(\n",
    "                storage_type.value,\n",
    "                documents,\n",
    "                self.embedding_model,\n",
    "                index_name\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Datastore initialization failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionInitializer.py (Test)\n",
    "from typing import List, Dict, Optional, Any\n",
    "import time\n",
    "from ScoringMetric import ScoringMetric\n",
    "from QuestionAnsweringPipeline import QuestionAnsweringPipeline\n",
    "from TemplateManager import TemplateManager\n",
    "from GroundTruthManager import GroundTruthManager\n",
    "\n",
    "class QuestionInitializer:\n",
    "    \"\"\"Manages question answering workflow including templates, ground truth, and pipeline execution.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 datastore: Any,\n",
    "                 model: Any,\n",
    "                 embedding_model: Any,\n",
    "                 template_path: str = \"templates.json\",\n",
    "                 ground_truth_path: str = \"ground_truth.json\"):\n",
    "        \"\"\"Initialize question processor with models and paths.\"\"\"\n",
    "        self.datastore = datastore\n",
    "        self.model = model\n",
    "        self.embedding_model = embedding_model\n",
    "        self.template_path = template_path\n",
    "        self.ground_truth_path = ground_truth_path\n",
    "        \n",
    "        # Initialize components\n",
    "        self.template_manager = TemplateManager(template_path)\n",
    "        self.scoring_metric = ScoringMetric(embedding_model)\n",
    "        \n",
    "    def _load_template(self, template_name: str = \"default\") -> str:\n",
    "        \"\"\"Load specific template.\"\"\"\n",
    "        template = self.template_manager.get_template(template_name)\n",
    "        print(f\"Loaded template: {template}\")\n",
    "        return template\n",
    "        \n",
    "    def _get_ground_truth(self, \n",
    "                         questions: List[str], \n",
    "                         use_ground_truth: bool = False) -> Optional[Dict[str, str]]:\n",
    "        \"\"\"Retrieve ground truth answers if available.\"\"\"\n",
    "        if not use_ground_truth:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            ground_truth_manager = GroundTruthManager(self.ground_truth_path)\n",
    "            return {q: ground_truth_manager.get_answer(q) for q in questions}\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load ground truth: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def process_questions(self,\n",
    "                         questions: List[str],\n",
    "                         use_ground_truth: bool = False,\n",
    "                         template_name: str = \"default\") -> float:\n",
    "        \"\"\"Process questions through pipeline and return execution time.\"\"\"\n",
    "        try:\n",
    "            # Load template and ground truth\n",
    "            template = self._load_template(template_name)\n",
    "            ground_truth = self._get_ground_truth(questions, use_ground_truth)\n",
    "            \n",
    "            # Initialize and run pipeline\n",
    "            pipeline = QuestionAnsweringPipeline(\n",
    "                self.datastore,\n",
    "                self.model,\n",
    "                template,\n",
    "                self.scoring_metric,\n",
    "                embeddings = self.embedding_model\n",
    "            )\n",
    "            \n",
    "            # Process questions and measure time\n",
    "            start_time = time.time()\n",
    "            pipeline.answer_questions(questions, ground_truth)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            return processing_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to process questions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Driver cells that use the full functionality of the stable backup Python files '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGInitializer Driver\n",
    "from RAGInitializer import LLMType, EmbeddingType, RAGConfig, initialize_rag_components\n",
    "\n",
    "'''\n",
    "Available choices for language models (LLMs)\n",
    "GPT: \n",
    "    0 - gpt-4o\n",
    "Ollama:\n",
    "    0 - llama3.1:8b-instruct-q5_K_M\n",
    "    1 - llama3.2:latest\n",
    "    2 - mistral-nemo:12b-instruct-2407-q5_K_M\n",
    "\n",
    "Available choices for embedding models\n",
    "GPT:\n",
    "    0 - text-embedding-3-small\n",
    "    1 - text-embedding-3-large\n",
    "Ollama:\n",
    "    0 - nomic-embed-text\n",
    "    1 - mxbai-embed-large\n",
    "    2 - all-minilm\n",
    "    3 - snowflake-arctic-embed\n",
    "Sentence Transformer:\n",
    "    0 - all-MiniLM-L6-v2\n",
    "    1 - all-MiniLM-L12-v2\n",
    "    2 - all-mpnet-base-v2\n",
    "    3 - all-distilbert-base-v2\n",
    "    4 - multi-qa-mpnet-base-dot-v1\n",
    "'''\n",
    "\n",
    "# Initialize with configuration\n",
    "config = RAGConfig(\n",
    "    env_path = r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\EDQP_RAG_Model\\env_variables.env\",\n",
    "    llm_type=LLMType.OLLAMA, # GPT or OLLAMA\n",
    "    embedding_type=EmbeddingType.SENTENCE_TRANSFORMER, # OLLAMA, GPT, or SENTENCE_TRANSFORMER\n",
    "    llm_index=1,\n",
    "    embedding_index=2\n",
    ")\n",
    "\n",
    "model, embeddings, dimensions, selected_llm, selected_embedding_model, model_manager = initialize_rag_components(config)\n",
    "if model and embeddings and dimensions:\n",
    "    print(\"\\nRAG components successfully initialized.\")\n",
    "    print(f\"Model: {selected_llm}\")\n",
    "    print(f\"Embeddings: {selected_embedding_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChunkingInitializer Driver\n",
    "\n",
    "from ChunkingInitializer import ChunkingInitializer\n",
    "from ChunkingMethod import ChunkingMethod\n",
    "\n",
    "processor = ChunkingInitializer(\n",
    "    source_path=r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\Source_Documents\\MCDP_1_Warfighting.pdf\",\n",
    "    chunking_method=ChunkingMethod.PAGE,\n",
    "    enable_preprocessing=False,\n",
    "    model_name=selected_llm,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "try:\n",
    "    documents = processor.process()\n",
    "    \n",
    "    # Output results\n",
    "    for doc in documents:\n",
    "        print(f\"Chunk metadata: {doc.metadata}\")\n",
    "        print(f\"Chunk content: {doc.page_content[:200]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Processing failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if documents:\n",
    "    print(\"\\nDocument processing completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatastoreInitializer Driver\n",
    "\n",
    "from DatastoreInitializer import DatastoreInitializer, StorageType\n",
    "\n",
    "datastore_manager = DatastoreInitializer(\n",
    "    doc_name='test-datastore',\n",
    "    pinecone_api_key=model_manager.get_pinecone_api_key(),\n",
    "    dimensions=dimensions,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Set up datastore\n",
    "    datastore = datastore_manager.setup_datastore(\n",
    "        storage_type=StorageType.PINECONE_EXISTING,\n",
    "        documents=documents,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuestionInitializer Driver\n",
    "\n",
    "from QuestionInitializer import QuestionInitializer\n",
    "\n",
    "# QuestionInitializer Driver\n",
    "processor = QuestionInitializer(\n",
    "    datastore=datastore,\n",
    "    model=model,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "# Define questions\n",
    "template_name = \"detailed\"  # default, short, or detailed\n",
    "questions = [\"According to the text, what is the definition of war?\",\n",
    "             \"What does the text say about uncertainty in warfare?\",\n",
    "             \"How does the text define the spectrum of conflict? What are the various factors involved?\",\n",
    "             \"What are centers of gravity and critical vulnerabilities in warfare?\"\n",
    "            ]\n",
    "\n",
    "try:\n",
    "    # Process questions and get execution time\n",
    "    processing_time = processor.process_questions(\n",
    "        questions=questions,\n",
    "        use_ground_truth=False,\n",
    "        template_name=template_name  # Added template parameter\n",
    "    )\n",
    "    print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Full backup files are stable and functional. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and embeddings selection (Stable)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ModelManager import ModelManager\n",
    "from ComputeResourceManager import ComputeResourceManager\n",
    "\n",
    "# Load environment variables, model, and embeddings, including API keys for OpenAI and Pinecone (Test)\n",
    "env_path = r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\EDQP_RAG_Model\\env_variables.env\"\n",
    "model_manager = ModelManager(env_path)\n",
    "resource_manager = ComputeResourceManager().get_compute_settings()\n",
    "\n",
    "'''\n",
    "Available choices for language models (LLMs)\n",
    "GPT: \n",
    "    0 - gpt-4o\n",
    "Ollama:\n",
    "    0 - llama3.1:8b-instruct-q5_K_M\n",
    "    1 - llama3.2:latest\n",
    "    2 - mistral-nemo:12b-instruct-2407-q5_K_M\n",
    "\n",
    "Available choices for embedding models\n",
    "GPT:\n",
    "    0 - text-embedding-3-small\n",
    "    1 - text-embedding-3-large\n",
    "Ollama:\n",
    "    0 - nomic-embed-text\n",
    "    1 - mxbai-embed-large\n",
    "    2 - all-minilm\n",
    "    3 - snowflake-arctic-embed\n",
    "Sentence Transformer:\n",
    "    0 - all-MiniLM-L6-v2\n",
    "    1 - all-MiniLM-L12-v2\n",
    "    2 - all-mpnet-base-v2\n",
    "    3 - all-distilbert-base-v2\n",
    "    4 - multi-qa-mpnet-base-dot-v1\n",
    "'''\n",
    "\n",
    "# Configuration settings for selecting LLM and embedding scheme\n",
    "config = {\n",
    "    \"selected_llm_type\": \"ollama\",  # Options: \"gpt\" or \"ollama\"\n",
    "    \"selected_embedding_scheme\": \"gpt\"  # Options: \"ollama\", \"gpt\", or \"sentence_transformer\"\n",
    "}\n",
    "# Select specific indices from llm_choices and embedding_choices\n",
    "select_llm = 1\n",
    "select_embed = 1\n",
    "\n",
    "# Validate user selections for LLM type and embedding scheme\n",
    "model_manager.validate_selection(config[\"selected_llm_type\"], model_manager.llm_choices.keys())\n",
    "model_manager.validate_selection(config[\"selected_embedding_scheme\"], model_manager.embedding_choices.keys())\n",
    "\n",
    "# Select the LLM and embedding model based on configuration and indices\n",
    "selected_llm = (\n",
    "    model_manager.llm_choices[config[\"selected_llm_type\"]]\n",
    "    if config[\"selected_llm_type\"] == \"gpt\"\n",
    "    else model_manager.llm_choices[config[\"selected_llm_type\"]][select_llm]\n",
    ")\n",
    "selected_embedding_model = model_manager.embedding_choices[config[\"selected_embedding_scheme\"]][select_embed]\n",
    "\n",
    "# Load the selected language model and embeddings model\n",
    "\n",
    "model = model_manager.load_model(config[\"selected_llm_type\"], selected_llm, resource_manager)\n",
    "embeddings = model_manager.load_embeddings(config[\"selected_embedding_scheme\"], selected_embedding_model)\n",
    "\n",
    "# Dynamically determine the dimensions of the embeddings for compatibility with the index\n",
    "dimensions = model_manager.determine_embedding_dimensions(embeddings)\n",
    "\n",
    "# Output confirmation of selected options\n",
    "print(f\"\\nSelected LLM: {selected_llm}\")\n",
    "print(f\"Selected Embedding Scheme: {config['selected_embedding_scheme']}\")\n",
    "print(f\"Selected Embedding Model: {selected_embedding_model}\")\n",
    "print(f\"Embedding dimensions: {dimensions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR and Chunking (Stable)\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from OCREnhancedPDFLoader import OCREnhancedPDFLoader\n",
    "from TextPreprocessor import TextPreprocessor\n",
    "from ChunkingManager import ChunkingMethod, process_document\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Parameters to specify the file path and preprocessing settings.\n",
    "# source_path is the location of the PDF file for OCR and NLP processing.\n",
    "source_path = r\"C:\\Users\\docsp\\Desktop\\AI_ML_Folder\\Python_Practice_Folder\\Natural_Language_Processing\\Source_Documents\\ED_Basic_Course_Guide\\3.1.3_Program_Funding.pdf\"\n",
    "enable_preprocessing = False  # Flag to enable or skip document preprocessing; set to True only for text documents\n",
    "\n",
    "# Load text documents from the specified PDF file and enhance with OCR.\n",
    "# Attempts to load the document and perform OCR; exits if an error occurs.\n",
    "try:\n",
    "    print(\"Loading documents with OCR enhancement...\")\n",
    "    loader = OCREnhancedPDFLoader(source_path)\n",
    "    text_documents = loader.load()\n",
    "    print(f\"Loaded {len(text_documents)} documents with OCR enhancement\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading documents: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Process documents based on the enable_preprocessing flag.\n",
    "# Applies preprocessing to each document or skips it, then saves processed documents.\n",
    "try:\n",
    "    if enable_preprocessing:\n",
    "        print(\"Preprocessing documents...\")\n",
    "        preprocess = TextPreprocessor()\n",
    "        processed_documents = [\n",
    "            Document(\n",
    "                page_content=preprocess.preprocess(doc.page_content),\n",
    "                metadata={**doc.metadata, \"preprocessing\": \"applied\"}\n",
    "            ) for doc in text_documents\n",
    "        ]\n",
    "    else:\n",
    "        print(\"Skipping preprocessing...\")\n",
    "        processed_documents = [\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={**doc.metadata, \"preprocessing\": \"skipped\"}\n",
    "            ) for doc in text_documents\n",
    "        ]\n",
    "except Exception as e:\n",
    "    print(f\"Error in document processing: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Select the chunking method and process the document.\n",
    "# The chunking method and parameters control how documents are split into smaller parts for analysis.\n",
    "chunking_method = ChunkingMethod.PAGE  # Choose page-based (.PAGE) or semantic-based (.SEMANTIC) chunking\n",
    "\n",
    "embedding_method = \"semantic\"\n",
    "if embedding_method == \"semantic\":\n",
    "    embedding_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "elif embedding_method == \"page\":\n",
    "    embedding_model = selected_embedding_model\n",
    "\n",
    "documents = process_document(\n",
    "    source_path=source_path,\n",
    "    method=chunking_method,\n",
    "    enable_preprocessing=enable_preprocessing,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    similarity_threshold=0.85,\n",
    "    model_name=selected_llm,\n",
    "    embedding_model=selected_embedding_model\n",
    ")\n",
    "\n",
    "# Output the processed documents\n",
    "print(f\"Processed {len(documents)} document chunks\")\n",
    "for doc in documents:\n",
    "    print(f\"Chunk metadata: {doc.metadata}\")\n",
    "    print(f\"Chunk content: {doc.page_content[:200]}...\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datastore and Pinecone index setup (Stable)\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from PineconeManager import PineconeManager\n",
    "\n",
    "# Define constants\n",
    "PINECONE_NEW = 0\n",
    "PINECONE_ADD = 1\n",
    "PINECONE_EXISTING = 2\n",
    "LOCAL_STORAGE = 3\n",
    "\n",
    "# Configure new Pinecone index, add to an existing index, or use an existing index.\n",
    "doc_name = 'edqp-test'\n",
    "pinecone_api_key = model_manager.get_pinecone_api_key()\n",
    "data_storage = PINECONE_NEW\n",
    "\n",
    "if data_storage == PINECONE_EXISTING:\n",
    "    documents = None\n",
    "\n",
    "try:\n",
    "    # Initialize Pinecone client and PineconeManager\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    manager = PineconeManager(pc, pinecone_api_key, dimensions, selected_embedding_model)\n",
    "\n",
    "    # Create a unique index name\n",
    "    index_name = f\"{doc_name}-{selected_embedding_model}\".lower()\n",
    "    print(f\"Active Index: {index_name}\")\n",
    "\n",
    "    # Set up Pinecone index if required\n",
    "    if data_storage == PINECONE_NEW:\n",
    "        spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        index = manager.setup_index(index_name, spec)\n",
    "\n",
    "    # Set up the datastore\n",
    "    datastore = manager.setup_datastore(data_storage, documents, embeddings, index_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer the questions (Stable)\n",
    "\n",
    "from ScoringMetric import ScoringMetric\n",
    "from QuestionAnsweringPipeline import QuestionAnsweringPipeline\n",
    "from TemplateManager import TemplateManager\n",
    "from GroundTruthManager import GroundTruthManager\n",
    "\n",
    "# Initialize TemplateManager with a JSON file and retrieve a specific template\n",
    "template_manager = TemplateManager(\"templates.json\")\n",
    "template = template_manager.get_template(\"default\")\n",
    "print(template)\n",
    "\n",
    "# List to store questions for model to answer\n",
    "questions = [\"What is the definition of life-cycle cost?\",\n",
    "             \"Compare and contrast the analogy and parametric methods of cost estimation.\",\n",
    "             \"Compare and contrast total ownership cost and life-cycle cost.\",\n",
    "             \"What is learning curve theory?\",\n",
    "             \"Define the terms budget authority, committment, obligation, expedniture, and outlay.\"\n",
    "             ]  \n",
    "\n",
    "# Retrieve ground truth answers for the questions\n",
    "ground_truth_available = False\n",
    "if ground_truth_available:\n",
    "    ground_truth_manager = GroundTruthManager(\"ground_truth.json\")\n",
    "    ground_truth = {q: ground_truth_manager.get_answer(q) for q in questions}\n",
    "else:\n",
    "    ground_truth = None\n",
    "\n",
    "# Initialize ScoringMetric with the selected embedding model\n",
    "scoring_metric = ScoringMetric(selected_embedding_model)\n",
    "\n",
    "# Initialize question and answer pipeline\n",
    "pipeline = QuestionAnsweringPipeline(datastore, model, template, scoring_metric)\n",
    "\n",
    "# Answer questions\n",
    "model_output = pipeline.answer_questions(questions, ground_truth)\n",
    "print(f\"Total processing time: {model_output:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
